{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set.\n",
    "\n",
    "The first change that I made was to add the recall, precision, f-beta, and confusion matrix metrics so that I could get \n",
    "a good idea of the current performance. I also added K-Fold cross-validation with a value of n=5. I chose 5 since it \n",
    "would perform 5 iterations which seemed like a good balance of multiple attempts without causing the code to run for a \n",
    "long time. It also meant that 20% of the data would be held for testing and 80% would be used for training. These\n",
    "values are similar to the original ratio in the provided data and also show a good tradeoff between training data size\n",
    "and test data size.\n",
    "\n",
    "The base results were:\n",
    "\n",
    "Avg. Recall: 0.571\n",
    "\n",
    "Avg. Precision: 0.889\n",
    "\n",
    "Avg. F-Beta: 0.696\n",
    "\n",
    "Avg. ROC AUC: 0.955\n",
    "\n",
    "Time to run:  0:04:56.208785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634'\n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637'\n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "\n",
    "start = datetime.now()                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "8. Text Cleanup\n",
    "\n",
    "a. I attempted many text cleanup methods. The ones included in the code are replacing quotes with an empty string,\n",
    "replacing all punctuation with spaces, reducing multiples consecutive spaces to one, and making the text lowercase. I\n",
    "had also tried removing stopwords using the built in 'english' stop word dictionary in the CountVectorizer. I had also \n",
    "tried replacing all punctuation with empty strings.\n",
    "\n",
    "With replacing all punctuation with empty strings:\n",
    "\n",
    "Avg. Recall: 0.566\n",
    "\n",
    "Avg. Precision: 0.891\n",
    "\n",
    "Avg. F-Beta: 0.692\n",
    "\n",
    "Avg. ROC AUC: 0.954\n",
    "\n",
    "Time to run:  0:03:37.461214\n",
    "\n",
    "With the features that were left in:\n",
    "\n",
    "Avg. Recall: 0.571\n",
    "\n",
    "Avg. Precision: 0.889\n",
    "\n",
    "Avg. F-Beta: 0.695\n",
    "\n",
    "Avg. ROC AUC: 0.955\n",
    "\n",
    "Time to run:  0:03:49.056966\n",
    "\n",
    "Adding stopwords to the features that were left in:\n",
    "\n",
    "Avg. Recall: 0.559\n",
    "\n",
    "Avg. Precision: 0.894\n",
    "\n",
    "Avg. F-Beta: 0.688\n",
    "\n",
    "Avg. ROC AUC: 0.949\n",
    "\n",
    "Time to run:  0:03:38.880159\n",
    "\n",
    "As you can see, using spaces provided better performance than empty strings and adding stopwords made the results worse.\n",
    "The use of stopwords making the results worse surprised me at first as this can typically help remove noise, but\n",
    "it does seem that the stopword lists may include some terms that would not be considered as stopwords in all domains\n",
    "as referenced in the docs:  https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove newline, tab tokens, and many forms of punctuation\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "# Replace quote with nothing\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"'\", \"\"))\n",
    "# Idea borrowed from https://stackoverflow.com/questions/34860982/replace-the-punctuation-with-whitespace\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))))\n",
    "# Collapse multiple spaces to one\n",
    "comments['comment'] = comments['comment'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "# Make every string lowercase\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279             Iraq is not good  ===  ===  USA is bad   \n",
       "2702703      ____ fuck off you little asshole. If you wan...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332      == renault ==  you sad little bpy for drivin...\n",
       "6545351      == renault ==  you sad little bo for driving...\n",
       "7977970    34, 30 Nov 2004 (UTC)  ::Because you like to a...\n",
       "8359431    `  ::You are not worth the effort. You are arg...\n",
       "8724028    Yes, complain to your rabbi and then go shoot ...\n",
       "8845700                     i am using the sandbox, ass wipe\n",
       "8845736      == GOD DAMN ==  GOD DAMN it fuckers, i am us...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veronica\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.957\n"
     ]
    }
   ],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "# get different data groups\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "# Tuning data\n",
    "dev_comments = comments.query(\"split=='dev'\")\n",
    "# Test/Train data when using KFold\n",
    "non_dev_comments = comments.query(\"split=='test' or split=='train'\")\n",
    "\n",
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "12. Parameter Tuning (Part 1)\n",
    "\n",
    "e. For the parameter tuning phase, I started with the documentation for the model and searched around on the internet to\n",
    "see what parameters could make a difference and the typical values used. I came up with the set below. I did reduce set\n",
    "the cv on the GridSearch to 3 to reduce the runtime, but it was able to improve the results.\n",
    "\n",
    "The previous results versus post tuning:\n",
    "\n",
    "Avg. Recall: 0.452 v .613  -- improved by .161\n",
    "\n",
    "Avg. Precision: 0.946 v .900 -- decreased by .46\n",
    "\n",
    "Avg. F-Beta: 0.611 v .729 -- increased by .118\n",
    "\n",
    "Avg. ROC AUC: 0.952 v .965 -- increased by .013\n",
    "\n",
    "Time to run:  0:09:24.646272 v 0:05:41.022994 -- time decreased by over 3.5 minutes\n",
    "\n",
    "While it did drop down the high precision that I originally found with the classifier, the tuning did improve the recall,\n",
    "F-beta, and ROC AUC significantly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # parameters to tune\n",
    "    parameters = {\n",
    "        'clf__alpha': [.00000001, .000001, .0001, .01, 1, 100],\n",
    "        'clf__loss': ['log', 'modified_huber'],\n",
    "        'clf__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'clf__max_iter': [1000, 2000],\n",
    "        'clf__n_iter_no_change': [5, 10],\n",
    "        'clf__class_weight': ['balanced', None],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Feature Extraction and 11. Modeling the Data\n",
    "\n",
    "b. Most of the work I did around features revolved around using words versus character n-grams and the size of the n-grams.\n",
    "I first tried word features and experimented with unigrams, unigrams and bigrams, and unigrams, bigrams, and trigrams.\n",
    "Ultimately, only using unigrams provided the best results:\n",
    "\n",
    "Avg. Recall: 0.581\n",
    "\n",
    "Avg. Precision: 0.891\n",
    "\n",
    "Avg. F-Beta: 0.703\n",
    "\n",
    "Avg. ROC AUC: 0.958\n",
    "\n",
    "Time to run:  0:02:35.807145\n",
    "\n",
    "I then tried running it with only word boundary character n-grams. I tried bigrams, trigrams, 4-grams, and 5-grams. The \n",
    "best results were produced by the 4-grams run.\n",
    "\n",
    "Avg. Recall: 0.611\n",
    "\n",
    "Avg. Precision: 0.890\n",
    "\n",
    "Avg. F-Beta: 0.725\n",
    "\n",
    "Avg. ROC AUC: 0.963\n",
    "\n",
    "Time to run:  0:04:51.814777\n",
    "\n",
    "I then combined word unigrams with 4-grams and 5-grams to see what combinations worked best and the best results came \n",
    "from the unigrams with 4-grams.\n",
    "\n",
    "Avg. Recall: 0.612\n",
    "\n",
    "Avg. Precision: 0.893\n",
    "\n",
    "Avg. F-Beta: 0.726\n",
    "\n",
    "Avg. ROC AUC: 0.964\n",
    "\n",
    "Time to run:  0:05:21.840541\n",
    "\n",
    "I then tried to add the length of the comments, which made the results worse. I also tried upping the number of features\n",
    "for the word and character n-grams to 20,000 and the results were worse.\n",
    "\n",
    "After completing all of the other steps, I returned to each step in turn to see if I could make any other improvements.\n",
    "Ultimately I added a feature capturing the logged_in field of the data and found that adding that improved the results.\n",
    "These results were found after performing all of the other steps, so the improvements are not solely from adding this feature.\n",
    "\n",
    "Avg. Recall: 0.621\n",
    "\n",
    "Avg. Precision: 0.899\n",
    "\n",
    "Avg. F-Beta: 0.734\n",
    "\n",
    "Avg. ROC AUC: 0.964\n",
    "\n",
    "Time to run:  0:05:56.381002\n",
    "\n",
    "The features included in the final system are word unigrams, character 4-grams, and the boolean value of the\n",
    "logged_in field.\n",
    "\n",
    "d. I tried 4 different models. The first was the LogisticRegression that was part of the base code.\n",
    "\n",
    "Avg. Recall: 0.612\n",
    "\n",
    "Avg. Precision: 0.893\n",
    "\n",
    "Avg. F-Beta: 0.726\n",
    "\n",
    "Avg. ROC AUC: 0.964\n",
    "\n",
    "Time to run:  0:05:21.840541\n",
    "\n",
    "Multinomial Naive Bayes\n",
    "\n",
    "Avg. Recall: 0.602\n",
    "\n",
    "Avg. Precision: 0.842\n",
    "\n",
    "Avg. F-Beta: 0.702\n",
    "\n",
    "Avg. ROC AUC: 0.932\n",
    "\n",
    "Time to run:  0:04:51.760757\n",
    "\n",
    "RandomForest\n",
    "\n",
    "Avg. Recall: 0.522\n",
    "\n",
    "Avg. Precision: 0.888\n",
    "\n",
    "Avg. F-Beta: 0.658\n",
    "\n",
    "Avg. ROC AUC: 0.914\n",
    "\n",
    "Time to run:  0:08:07.777086\n",
    "\n",
    "SGDClassifier\n",
    "\n",
    "Avg. Recall: 0.452\n",
    "\n",
    "Avg. Precision: 0.946\n",
    "\n",
    "Avg. F-Beta: 0.611\n",
    "\n",
    "Avg. ROC AUC: 0.952\n",
    "\n",
    "Time to run:  0:09:24.646272\n",
    "\n",
    "Ultimately, I chose the SGDClassifier. These results actually seem to be worse than the LogisticRegression, \n",
    "but the difference in the precision was significant and none of the other models provided a value near it. \n",
    "I figured I could try it and see what would happen with tuned parameters. If I was looking to maximize the ROC AUC score,\n",
    "I would have kept the LogisticRegression model, but I wanted to see if I could use the SGDClassifier and get better results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # Combine word and character features\n",
    "    word_and_char = FeatureUnion([\n",
    "        ('vect_word', CountVectorizer(max_features=10000, analyzer='word', ngram_range=(1, 1))),\n",
    "        # Borrowed from https://stackoverflow.com/questions/39121104/how-to-add-another-feature-length-of-text-to-current-bag-of-words-classificati\n",
    "        ('vect_char', CountVectorizer(max_features=10000, analyzer='char_wb', ngram_range=(4, 4)))\n",
    "    ])\n",
    "\n",
    "    clf = Pipeline([\n",
    "        # Combine word/char features with the logged_in column\n",
    "        ('all', FeatureUnion([\n",
    "            ('comments', Pipeline([\n",
    "                ('extract_field', FunctionTransformer(lambda x: x['comment'], validate=False)),\n",
    "                ('vects', word_and_char),\n",
    "                ('tfidf', TfidfTransformer(norm='l2'))\n",
    "            ])),\n",
    "            ('login', Pipeline([\n",
    "                ('extract_field', FunctionTransformer(lambda x: x['logged_in'][:, np.newaxis], validate=False)),\n",
    "                ('encoder', OneHotEncoder())\n",
    "            ]))\n",
    "        ])),\n",
    "        # Classify using the parameters that were found to be the best values\n",
    "        # The search.fit below can be commented out as these values were found using that and it takes ~3.5 hours\n",
    "        ('clf', SGDClassifier(alpha=.0001, class_weight=None, loss='modified_huber', max_iter=1000, n_iter_no_change=10, penalty='elasticnet', random_state=5))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "12. Parameter Tuning (Part 2)\n",
    "\n",
    "There parameters in the SGDClassifier above are the values that were produced by GridSearchCV as the best parameters.\n",
    "The code below can be commented out to not perform the parameter tuning steps which take about 3.5 hours."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # Find best parameters\n",
    "    # search = GridSearchCV(clf, parameters, cv=3, verbose=10)\n",
    "    # search.fit(dev_comments['comment'], dev_comments['attack'])\n",
    "    # \n",
    "    # # Print out the best score and parameter set\n",
    "    # print(\"Best Score: %.3f\" %search.best_score_)\n",
    "    # print(\"Best parameters set:\")\n",
    "    # \n",
    "    # best_parameters = search.best_estimator_.get_params()\n",
    "    # for param_name in sorted(parameters.keys()):\n",
    "    #     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # Set up KFold\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    fbetas = []\n",
    "    roc_aucs = []\n",
    "\n",
    "    i =  1\n",
    "    # For each split in KFold\n",
    "    for training_data_indices, test_data_indices in kf.split(non_dev_comments):\n",
    "        print(\"**********************************************************\")\n",
    "        print(\"Test Run: \" + i)\n",
    "        # Get training and test data\n",
    "        training_data = non_dev_comments.iloc[training_data_indices]\n",
    "        test_data = non_dev_comments.iloc[test_data_indices]\n",
    "\n",
    "        # Fit the training data\n",
    "        clf = clf.fit(training_data, training_data['attack'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Metrics (Part 1)\n",
    "\n",
    "f. The metrics provided a lot of information. The ROC AUC metric was a bit confusing at first but some internet research\n",
    "helped to clarify it. Adding in the precision, recall, and confusion matrix was very useful as they provided a very clear \n",
    "picture of where the classifier could improve. They also helped to provide information on how changes could be influencing\n",
    "the performance in one direction or another. As seen by the results for the models, based solely on the ROC AUC score, I \n",
    "would have chosen LogisticRegression but the high precision suggested that SDGClassifier may perform well after being tuned.\n",
    "Having a variety of metric allowed for a much deeper understanding of the results and how they were affected by the decisions \n",
    "that I made.\n",
    "\n",
    "I did add cross validation. I used a K-Fold method with n=5 that I provided all non tuning comments to. As mentioned above\n",
    "the value of 5 was chosen to balance out the size of training versus test data and the amount of time it would take to run\n",
    "multiple iterations. I tracked the recall, precision, f-beta, and roc auc score for each iteration and then averaged them\n",
    "in order to provide the metrics seen throughout this notebook. I did not average the confusion matrix as it didn't make \n",
    "sense as something to be averaged, but do print it for each iteration of the K-Fold."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # Predict the test data\n",
    "        predictions = clf.predict(test_data)\n",
    "        # Get the precision, recall, and fbeta\n",
    "        (precision, recall, fbeta, support) = precision_recall_fscore_support(test_data['attack'], predictions, average='binary')\n",
    "        print('Recall: %.3f' %recall)\n",
    "        print('Precision: %.3f' %precision)\n",
    "        print('F-Beta: %.3f' %fbeta)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        fbetas.append(fbeta)\n",
    "        \n",
    "        # Get the confusion matrix\n",
    "        conf_matrix = confusion_matrix(test_data['attack'], predictions)\n",
    "        print('Confusion Matrix:\\n', conf_matrix)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        # Get the roc auc score\n",
    "        auc = roc_auc_score(test_data['attack'], clf.predict_proba(test_data)[:, 1])\n",
    "        print('Test ROC AUC: %.3f' %auc)\n",
    "        roc_aucs.append(auc)\n",
    "        \n",
    "        i += 1\n",
    "        print(\"**********************************************************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Metrics (Part 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Average the recall, precision, F-beta, and roc auc\n",
    "    print('Avg. Recall: %.3f' %np.mean(recalls))\n",
    "    print('Avg. Precision: %.3f' %np.mean(precisions))\n",
    "    print('Avg. F-Beta: %.3f' %np.mean(fbetas))\n",
    "    print('Avg. ROC AUC: %.3f' %np.mean(roc_aucs))\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "# Find time to run to understand performance from that perspective\n",
    "time = end - start\n",
    "print(\"Time to run: \", time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "c. No optimizations were performed in this code. A few tweaks were made though. The first change was the the classifier\n",
    "does not operate on just the column field but the entire data set so that it can access the logged_in field. Also, when\n",
    "performing parameter turning I set cv to 3 so that the process would be faster due to the number of parameters I tuned.\n",
    "With the current setup it took about 3.5 hours to run through each possibility. For the most part, having a long runtime\n",
    "was not an issue so effort was not spent trying to optimize the system.\n",
    "\n",
    "g. Final metric v Original metrics\n",
    "\n",
    "Avg. Recall: 0.621 v .571 -- improved by .05\n",
    "\n",
    "Avg. Precision: 0.899 v .889 -- improved by .01\n",
    "\n",
    "Avg. F-Beta: 0.734 v .696 -- improved by .038\n",
    "\n",
    "Avg. ROC AUC: 0.964 v .955 -- improved by .009\n",
    "\n",
    "Time to run:  0:05:56.381002 v 0:04:56.208785 -- slower by 1 minute\n",
    "\n",
    "Every metric improved, except for runtime, which is expected since we are now doing K-Fold with n=5.\n",
    "\n",
    "These results come from the SGDClassifier model.\n",
    "\n",
    "\n",
    "h. The most interesting thing that I learned was all of the different ways that there are to approach a problem like this.\n",
    "Starting with the text cleanup and all the way through the parameter tuning, there were so many decisions to make that\n",
    "at times it was overwhelming to decide what to try to do to improve the performance. I hadn't grasped the amount of\n",
    "thought and decision making that goes into designing a classification system and tuning it to perform as best as it can.\n",
    "It was very interesting to try out different things and see how that decision affected the metrics and how some improved\n",
    "recall but hurt precision and others improved both but could take a very long time to run. There was a lot of time spent\n",
    "trying to balances all of the metrics and find the \"best\" classifier.\n",
    "\n",
    "i. The hardest thing to do in this project was identify what decisions would be helpful to improving the classifier. As \n",
    "mentioned above, there was many decisions to make at each step and trying to make the best one was very difficult. Looking\n",
    "through the scikit documentation and searching online provided many different ways of cleaning text data and many different\n",
    "models that could be used. Even after narrowing down the list to a handful of models, I had to decide which classifier seemed\n",
    "to be the best in order to run parameter training on it as there was not enough time to try tuning every model. I had to\n",
    "make these decisions with the information I had an not having a complete grasp on how each model worked. After choosing a model\n",
    "it was difficult to decide which parameters to tune and how to tune them. Given the sharp increase in runtime for every\n",
    "parameter that is tuned and each value to be tried, there was a need to reduce the size of the set. I ultimately found a\n",
    "set that could run in about 3.5 hours and tune a handful of the parameters that SGDClassifier has."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}